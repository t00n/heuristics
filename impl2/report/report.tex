\documentclass[a4paper,10pt]{article}
\usepackage[american]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[left=2.5cm,top=2cm,right=2.5cm,nohead,nofoot]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{float}
\usepackage{natbib}

\title{Heuristic Optimisation\\Implementation 2}
\author{Antoine Carpentier}

\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

\begin{document}

\maketitle

\section{Introduction}

In this project, we implement two stochastic local search algorithms to solve the problem of the permutation flowshop scheduling with sum weighted completion time objective. We run them on several instances of different sizes and analyse the scores of the solutions and run-time of our algorithms.

\section{Algorithms}

We implement two algorithms : an Iterative Greedy Algorithm (IGA) taken from \cite{panruiz2012} and a Genetic Algorithm inspired from \cite{tseng2010genetic}.

	\subsection{Iterative Greedy Algorithm}

	Our IGA works as follows : 

	\begin{enumerate}
		\item Generate an initial solution using the LR algorithm described in \cite{liu2001constructive}
		\item Apply the iterated RZ algorithm on the initial solution
		\item Store the initial solution as the best solution
		\item Store the initial solution as the current solution
		\item Repeat the following steps until $N$ seconds have elapsed
		\item Destroy $d$ random jobs from the solution
		\item Repair the solution by inserting those jobs one by one in the solution at the position that maximizes the score
		\item Apply the iterated RZ algorithm on the solution
		\item If the score of the new solution is better than the current solution, replace the current solution by the new solution
		\item If (9) is true and the score of the new solution is better than the best solution, replace the best solution by the new solution
		\item If (9) is false, use simulated annealing to replace the current solution by the new solution with a probability.
	\end{enumerate}

	We will now describe the non-trivial steps of this algorithm. \\

	\paragraph{} The iterated RZ algorithm use in steps (2) and (8) applies the RZ algorithm described in \cite{rajendran1997efficient} until the solution stops changing because it is a local optimum.

	\paragraph{} The value of the $d$ parameter of step (6) was taken from \cite{panruiz2012} and we obtained good results with it.

	\paragraph{} Step (7) uses a greedy algorithm to maximize the score of the solution each time a job is added.

	\paragraph{} Step (11) uses simulated annealing with a fixed temperature defined as
	$$ t = \lambda \frac{\sum_{j=1}^{n}\sum_{i=1}^{m}p_{ij}}{10mn} $$ where $\lambda$ is a parameter, $n$ is the number of jobs, $m$ is the number of machines and $p_{ij}$ is the time needed by job $j$ on machine $i$. \\
	The probability of keeping the new solution if it is worse is defined as 
	$$ p = e^{\frac{score of current solution - score of new solution}{t}}$$

	\subsection{Genetic Algorithm}

		Our genetic algorithm works as follow : 
		\begin{enumerate}
			\item Create a population of size $Ps$
			\item Store the best solution in the population
			\item Repeat the following steps until $N$ seconds have elapsed
			\item Apply the crossover operator $Ps$ times to create new children
			\item Decide for each new child if it must be included in the population
			\item Find the new best solution
			\item Apply the mutation operator on $Ps * Pm$ random solutions
		\end{enumerate}

		We describe steps (1), (3), (4) and (5) in the next sections.

		\subsubsection{Initial population}
			The initial population is chosen randomly. The size of the population $Ps$ was chosen empirically. We tried several larger sizes of population and the algorithm was too slow to obtain good solutions in a reasonable amount of time. Smaller sizes of population prevented the algorithm to explore the search space and find good solutions.

		\subsubsection{Crossover}
			Our crossover operator is based on the Orthogonal Arrays Crossover operator from \cite{tseng2010genetic}. Orthogonal arrays allow to find the best setting of factors in experiments. They provide a representative sample of experiments that guarantee that the combination of factors is uniformely distributed, that the levels for each factor appear the same number of times in a column and in $t$ several columns. For full explanation, the reader may refer to \cite{tseng2010genetic} or \cite{montgomery1991design}. \\
			We use it here to find the best combination of segments from the two parents to create the best child. \\
			First we segment the two parent solutions in $k$ parts that we choose randomly. $k$ is chosen to be 31 if the instance size is 100 and 15 if the instance size is 50 following the work of \cite{tseng2010genetic}. \\
			Using the orthogonal arrays with $k$ factors and two levels for each factor, we construct several children by taking each segment from the mother if the level is 1 and the father otherwise. \\
			Because it might create duplicates to take from both parents, we must repair the children. We replace every duplicate by the missing jobs in the order we find them in the mother solution. \\
			We then use the Taguchi method to find the best combination of factors and create a new child that we repair. \\
			We only keep the child with the best score among all the children we created.

		\subsubsection{Selection}
			We keep the two best solutions of the mother, the father and the child. \\
			In the case where the child is kept and it has been $TLS$ generations without improvement, we perform an iRZ on the child to improve it. As in the original paper, we do not improve the child if it is not better than its parents in order to save computation time. We choose $TLS$ to be 5 because we obtain good results with that value.

		\subsubsection{Mutation}
			The mutation operator uses the destruction and construction method from our IGA with same value for the parameter $d$.

\section{Results}

	\subsection{Average percentage deviation}

		\subsubsection{By instance}

		\begin{figure}[H]
			\centering
			\caption{Average percentage deviation from best known solutions by instance}
			\includegraphics[width=.8\textwidth]{apd_algo_instance.png}
		\end{figure}


		\begin{figure}[H]
			\centering
			\caption{Correlation plot between the average percentage deviation obtained by both algorithms on each instance}
			\includegraphics[width=.8\textwidth]{correlation.png}
		\end{figure}


		\subsubsection{By instance size}

		\begin{figure}[H]
			\centering
			\caption{Average percentage deviation from best known solutions by instance size}
			\includegraphics[width=.3\textwidth]{apd_algo_instance_size.png}
		\end{figure}

		instances size 50
		WilcoxonResult(statistic=161.0, pvalue=0.14138951131633032)
		instances size 100
		WilcoxonResult(statistic=1.0, pvalue=1.9209211049031396e-06)

	\subsection{Running time}

	\begin{figure}[H]
		\centering
		\caption{Distribution of time needed to be close of 2\% of best known solution}
		\includegraphics[width=.3\textwidth]{time_02.png}
	\end{figure}

	\begin{figure}[H]
		\centering
		\caption{Distribution of time needed to be close of 1\% of best known solution}
		\includegraphics[width=.3\textwidth]{time_01.png}
	\end{figure}

	\begin{figure}[H]
		\centering
		\caption{Distribution of time needed to be close of 0.5\% of best known solution}
		\includegraphics[width=.3\textwidth]{time_005.png}
	\end{figure}

	\begin{figure}[H]
		\centering
		\caption{Distribution of time needed to be close of 0.1\% of best known solution}
		\includegraphics[width=.3\textwidth]{time_001.png}
	\end{figure}


\section{Conclusion}


\bibliographystyle{apalike}
\bibliography{biblio.bib}


\end{document}

