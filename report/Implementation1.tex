\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}

\author{Antoine CARPENTIER}
\title{Heuristic optimization\\ \small Implementation exercises 1}

\bibliographystyle{apalike}

\begin{document}
\maketitle

\section{Exercise 1}

\subsection{Implementation}

The code is built modularly. The \texttt{construction\_search} function accepts pointers to functions that modifies its behavior. This allows to use several methods of construction search without code duplication.

\begin{itemize}
    \item The function \texttt{pick\_elem} picks a non-covered element
    \item The function \texttt{pick\_subset} picks a non-already-picked subset containing the element. This way no redundant subset is added during the solution construction
    \item The function \texttt{cost\_function} that computes the cost of a subset and is used in the \texttt{pick\_subset} function
\end{itemize}

To build the algorithms CH1 to CH4, the \texttt{construction\_search} function is parametrized as such : 

\begin{itemize}
    \item CH1 : with \texttt{random\_pick\_element}, \texttt{random\_pick\_subset} and \texttt{static\_cost} (unused)
    \item CH2 : with \texttt{random\_pick\_element}, \texttt{greedy\_pick\_subset} and \texttt{static\_cost}
    \item CH3 : with \texttt{random\_pick\_element}, \texttt{greedy\_pick\_subset} and \texttt{static\_cover\_cost}
    \item CH4 : with \texttt{random\_pick\_element}, \texttt{greedy\_pick\_subset} and \texttt{adaptive\_cover\_cost}
\end{itemize}

The function \texttt{random\_pick\_element} picks randomly a non covered element and is used everywhere.

The function \texttt{random\_pick\_subset} picks randomly an unused subset that contains the element.

The function \texttt{greedy\_pick\_subset} picks a unused subset with minimal that contains the element using the \texttt{cost\_function}.

The function \texttt{static\_cost} returns the raw cost of the subset.

The function \texttt{static\_cover\_cost} returns the raw cost of the subset divided by the number of elements in the subset.

The function \texttt{adaptive\_cover\_cost} returns the raw cost of the subset divided by the number of elements that it would add to the solution.

The redundancy elimination feature is provided by the function \texttt{eliminate\_redundancy} that uses the function \texttt{find\_redundant\_subsets} to compute every redundant subset and the remove the subset with highest cost using \texttt{static\_cost}.
A subset is detected as redundant by iterating on other subsets to find his every element.

\subsection{Results}

IPython and SciPy were used to process the results from the algorithms. You can find the \textit{ipynb} file in the archive. The instructions to use it are in the README file at the root of the archive.

These algorithms are very fast but not efficient. They stay easily stuck in local optima and implement no method to escape them. The worse algorithm in terms of both computation time and average percentage deviation is the CH1. The greedy algorithms implement a technique (even if it is basic) to find the best solutions but the CH1 only finds random solutions.

\subsubsection{Total computation times}

\begin{itemize}
    \item CH1 : 1.312 seconds
    \item CH2 : 1.023 seconds
    \item CH3 : 1.066 seconds
    \item CH4 : 1.048 seconds
    \item CH1 + RE : 3.113 seconds
    \item CH2 + RE : 1.700 seconds
    \item CH3 + RE : 1.415 seconds
    \item CH4 + RE : 1.170 seconds
\end{itemize}

We can see that CH1 is the slowest algorithm, both with and without redundancy elimination. The three others seem to be equal without redundancy elimination but the sophistication of the technique helps reducing the redundancy. The CH2 + RE is far slower than the CH2 than CH3 + RE is to the CH3. The fact that the evaluation function in CH3 and CH4 depends on the number of element of the subset prevents some redundant sets to be added to the solution.

\subsubsection{Average percentage deviation from best known solutions}

\begin{itemize}
    \item CH1 : 35.29\%
    \item CH2 : 0.27405\%
    \item CH3 : 0.3016\%
    \item CH4 : 0.1767\%
    \item CH1 + RE : 27.81\%
    \item CH2 + RE : 0.11592\%
    \item CH3 + RE : 0.14073\%
    \item CH4 + RE : 0.13197\%
\end{itemize}

The CH1 with or without redundancy elimination is very bad. The CH2 and CH3 seem equal but the CH4 is clearly better because it prevents redundancy more and thus reduce the number of subsets in the solution.

\subsubsection{Improvement of redundancy elimination}

We computed the percentage deviation of each instance with each algorithm with and without redundancy elimination and saw that 98\% of instances are improved by the redundancy elimination. The mean improvement is 0.15, the standard deviation is 0.10 and the maximum is 0.47.

We use a Wilcoxon signed-rank test to test if the mean of the percentage deviation of instances with or without redundancy is similar. We do not use the Student t-test because we can not assume that the population is normally distributed. We use the \texttt{scipy.stats.wilcoxon} function from \cite{scipywilcoxon} with a confidence interval of 9\% and the null hypothesis that the percentage deviations of the sample have the same mean.
The result is a pvalue of 1.28e-41 (less than 0.05) which means that we reject the null hypothesis and conclude that redundancy elimination has a significant influence on the performance of the algorithms.

\section{Exercise 2}

\subsection{Implementation}

The perturbative local search is implemented with the \texttt{perturbative\_search} function that is parametrized by the type of improvement (first or best) and a \texttt{cost\_function}. This is allows to perform several types of search. 
The \texttt{find\_improvement} function browse the neighbourhood consisting in removing one solution component randomly and completing the partial cover using a greedy static cover cost construction. It can either return the first improvement or the best depending on a parameter.

\subsection{Results}

\subsubsection{Total computation times}

\begin{itemize}
    \item CH1 + FI : 3.60 seconds
    \item CH4 + FI : 1.520 seconds
    \item CH1 + RE + FI : 3.587 seconds
    \item CH4 + RE + FI : 1.339 seconds
    \item CH1 + BI : 3.94 seconds
    \item CH4 + BI : 1.587 seconds
    \item CH1 + RE + BI : 3.642 seconds
    \item CH4 + RE + BI : 1.643 seconds
\end{itemize}

As it already was, the CH1 algorithm is very slow and neither the FI nor the BI can improve it. The BI seems slower because it must test more solutions than the FI. Applying the redundancy elimination before the BI or the FI seems to accelerate it.

\subsubsection{Average percentage deviation from best known solutions}

\begin{itemize}
    \item CH1 + FI : 26.474\%
    \item CH1 + RE + FI : 27.53\%
    \item CH4 + FI : 0.13006\%
    \item CH4 + RE + FI : 0.12505\%
    \item CH1 + BI : 0.3316\%
    \item CH1 + RE + BI : 26.957\%
    \item CH4 + BI : 0.08707\%
    \item CH4 + RE + BI : 0.09924\%
\end{itemize}

The CH1 is still the worse algorithm in terms of accuracy but the BI algorithm allows to correct it considerably. Applying the redundancy elimination sometimes increases and sometimes decreases the score so we can not draw any conclusion from this data only.

We computed the percentage deviation between methods without perturbative search, with FI perturbative search and with BI perturbative search and tested if the results are similar.

\subsubsection{Improvement of first improvement perturbative search}

First we compared the methods without perturbative search to the FI perturbative search. The fraction of instances improved due to first improvement is 64\%. The mean improvement is 0.09, the standard deviation is 0.13 and the maximum is 0.71, which means that overall the improvement is small.
Nevertheless we performed a Wilcoxon test and the pvalue of 5.1681488905994952e-28 means that we reject the null hypothesis that the results are similar and conclude that adding a first improvement perturbative search has a significant influence on the results.

\subsubsection{Improvement of best improvement perturbative search}

Second we compared the methods without perturbative search to the BI perturbative search. The fraction of instances improved is 72\%. The mean improvement is 7.56, the standard deviation is 16.52 and the maximum is 73.42, which means that the results are greatly improved.

We confirmed that performing a Wilcoxon test which gave us a pvalue of 1.02e-31 which means that we reject the null hypothesis and conclude that best improvement perturbative search has a significant influence on the results. The statistic value of the test was 33, confirming that the improvement is great.

\subsubsection{Comparison between first and best improvement}

Third we compared the first and best improvement methods. The fraction of instances improved when switching from first to best improvement is 67\%. The mean of 5.66 and the maximum of 57 confirm that the improvement is great.

The Wilcoxon test conducted obtains a pvalue of 6.01e-30 which means that we reject the null hypothesis and conclude that the best improvement method has a significant influence on the results.

\bibliography{main}
\end{document}
